% Simulated Annealing y Gradient Descent
@article{cai2021sa,
  title={Sa-gd: Improved gradient descent learning strategy with simulated annealing},
  author={Cai, Zhicheng},
  journal={arXiv preprint arXiv:2107.07558},
  year={2021}
}

% Newton 
@article{guo2014neural,
  title={Neural dynamics and Newton--Raphson iteration for nonlinear optimization},
  author={Guo, Dongsheng and Zhang, Yunong},
  journal={Journal of Computational and Nonlinear Dynamics},
  volume={9},
  number={2},
  pages={021016},
  year={2014},
  publisher={American Society of Mechanical Engineers}
}

@article{canterometodo,
  title={M{\'e}todo de Newton Raphson.},
  author={Cantero, Jose Enrique Vargas}
}


% Simulated Annealing
@article{busetti2003simulated,
  title={Simulated annealing overview},
  author={Busetti, Franco},
  journal={World Wide Web URL www. geocities. com/francorbusetti/saweb. pdf},
  volume={4},
  year={2003}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

%Newton Rhapson

@article{akram2015newton,
  title={Newton raphson method},
  author={Akram, Saba and Ann, Quarrat Ul},
  journal={International Journal of Scientific \& Engineering Research},
  volume={6},
  number={7},
  pages={1748--1752},
  year={2015}
}

@misc{ibm-gradient-descent,
    title = "What is Gradient Descent?",
    author = "IBM",
    year = "2023",
    month = "October",
    day = "31",
    url = "https://www.ibm.com/topics/gradient-descent",
    note = "Retrieved October 31, 2023"
}

%Vanilla 
@article{zhang2019gradient,
  title={Gradient descent based optimization algorithms for deep learning models training},
  author={Zhang, Jiawei},
  journal={arXiv preprint arXiv:1903.03614},
  year={2019}
}

@misc{donges-2021,
	author = {Donges, Niklas},
	month = {7},
	title = {{Gradient Descent in Machine Learning: A Basic Introduction}},
	year = {2021},
	url = {https://builtin.com/data-science/gradient-descent},
}